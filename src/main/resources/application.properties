server.port=8080

langchain4j.open-ai.chat-model.base-url=http://langchain4j.dev/demo/openai/v1
langchain4j.open-ai.chat-model.api-key=demo
langchain4j.open-ai.chat-model.model-name=gpt-4o-mini

langchain4j.ollama.chat-model.base-url=http://localhost:11434
langchain4j.ollama.chat-model.model-name=deepseek-r1:8b
# Temperature controls the randomness of the model's output.
langchain4j.ollama.chat-model.temperature=0.7 
# Timeout for the request to the model, in ISO-8601 duration format (PT60S = 60 seconds)
langchain4j.ollama.chat-model.timeout=PT60S

langchain4j.open-ai.chat-model.log-requests=true
langchain4j.open-ai.chat-model.log-responeses=true

#ali
langchain4j.community.dashscope.chat-model.api-key=sk-05a6a0b8315c4acf80427c033743f9f9
langchain4j.community.dashscope.chat-model.model-name=qwen-max


logging.level.root = debug